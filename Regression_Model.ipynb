{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbddc1e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Maybe you meant '==' or ':=' instead of '='? (1253643965.py, line 18)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mos.makedirs(\"plots\"), exist_ok=True\u001b[39m\n                          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree, export_text\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import shap\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Make directory if it doesn’t exist\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "os.makedirs(\"plots/rmplots\", exist_ok=True)\n",
    "\n",
    "ds = pd.read_csv(\"crop_yield_dataset.csv\")\n",
    "ds[\"Date\"] = pd.to_datetime(ds[\"Date\"])\n",
    "ds[\"Year\"] = ds[\"Date\"].dt.year\n",
    "ds[\"Month\"] = ds[\"Date\"].dt.month\n",
    "\n",
    "crops = [\"Tomato\", \"Wheat\", \"Corn\", \"Rice\", \"Barley\", \"Soybean\", \"Cotton\", \"Sugarcane\", \"Potato\", \"Sunflower\"]\n",
    "\n",
    "palette = sns.color_palette(\"tab10\", n_colors=len(crops))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Dataset Selection\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Select features (from EDA results)\n",
    "X = ds[[\"Soil_Quality\", \"Temperature\", \"Humidity\", \"N\", \"P\", \"K\", \"Soil_pH\", \"Wind_Speed\"]]\n",
    "y = ds[\"Crop_Yield\"]\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=36)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Linear Regression\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# Train regression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Linear R²:\", r2_score(y_test, y_pred)) # Measures how well the model explains variation in the data. Low R² means there are other factors not captured by the model (or a lot of randomness).\n",
    "print(\"Linear RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred))) # Measures the average prediction error, in the same units as the target\n",
    "print(\"Linear Coefficients:\", model.coef_) # Each number shows the effect of a 1-unit increase in that feature, while keeping others constant.\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Ridge Regression\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Linear regression + L2 penalty (shrinks coefficients).\n",
    "# Good when you have multicollinearity.\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "print(\"Ridge R²:\", r2_score(y_test, y_pred_ridge))\n",
    "print(\"Ridge RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_ridge)))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Lasso Regression\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Linear regression + L1 penalty (can shrink some coefficients all the way to 0).\n",
    "# Good for feature selection — it automatically “drops” unimportant predictors.\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "print(\"Lasso R²:\", r2_score(y_test, y_pred_lasso))\n",
    "print(\"Lasso RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_lasso)))\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# DescisionTreeRegressor\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Train Decision Tree\n",
    "tree = DecisionTreeRegressor(max_depth=3, random_state=36)  # max_depth controls tree size\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "# Plot Tree\n",
    "plt.figure(figsize=(20, 14))\n",
    "plot_tree(tree, feature_names=X.columns, filled=True, rounded=True)\n",
    "plt.savefig(\"plots/rmplots/decisiontree_tree.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print(\"Decision Tree R²:\", r2_score(y_test, y_pred))\n",
    "print(\"Decision Tree RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "# Feature importance values\n",
    "importances = tree.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=importances, y=features, palette=\"viridis\")\n",
    "plt.title(\"Feature Importance (DecisionTreeRegressor)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.savefig(\"plots/rmplots/decisiontree_feature_importance.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Exporting data from Tree\n",
    "\n",
    "# print(\"Features:\", tree.tree_.feature)      # indices of features at each split\n",
    "# print(\"Thresholds:\", tree.tree_.threshold)  # split points\n",
    "# print(\"Values:\", tree.tree_.value)          # predicted mean at each node\n",
    "\n",
    "# rules = export_text(tree, feature_names=list(X.columns))\n",
    "# print(rules)\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# RandomForestRegressor\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# Train RandomForest\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=None, random_state=36)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"RandomForest R²:\", r2_score(y_test, y_pred))\n",
    "print(\"RandomForest RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "\n",
    "#Plot\n",
    "importances = rf.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=importances, y=features, palette=\"crest\")\n",
    "plt.title(\"Feature Importance (RandomForestRegressor)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.savefig(\"plots/rmplots/randomforest_feature_importance.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# PDP -----------------------------------------------------------\n",
    "\n",
    "# Partial Dependence Plots (PDPs) show how changing one feature (e.g., Humidity) affects the predicted yield, keeping other features constant.\n",
    "PartialDependenceDisplay.from_estimator(rf, X, [\"Humidity\", \"Temperature\", \"Soil_Quality\", \"N\", \"P\",])\n",
    "plt.savefig(\"plots/rmplots/randomforest_PDP.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# SHAP ----------------------------------------------------------\n",
    "\n",
    "# Create an explainer for the model\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "X_sample = X_test.sample(100, random_state=36) if len(X_test) > 100 else X_test\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "shap.initjs()\n",
    "# Summary plot: global feature importance + direction\n",
    "shap.summary_plot(shap_values, X_sample, plot_type=\"bar\")\n",
    "plt.savefig(\"plots/rmplots/shap_summary.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# Beeswarm plot: shows how features push predictions up or down\n",
    "shap.summary_plot(shap_values, X_sample)\n",
    "plt.savefig(\"plots/rmplots/shap_bar.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# Force plot: explanation for a single prediction\n",
    "i = 0  # index of a row in your test set\n",
    "shap_html = shap.force_plot(\n",
    "    explainer.expected_value, shap_values[i], X_sample.iloc[i], matplotlib=False\n",
    ")\n",
    "shap.save_html(\"plots/rmplots/shap_force_plot.html\", shap_html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vLLM)",
   "language": "python",
   "name": "vllm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
